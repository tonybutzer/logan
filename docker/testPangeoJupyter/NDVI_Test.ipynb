{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import requests as r\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "from osgeo import gdal\n",
    "import pyproj\n",
    "from pyproj import Proj\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.shutil import copy\n",
    "from shapely.ops import transform\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Working Environment\n",
    "inDir = os.getcwd()\n",
    "os.chdir(inDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter NASA Earthdata Login Username \n",
      "(or create an account at urs.earthdata.nasa.gov): ········\n",
      "Enter NASA Earthdata Login Password: ········\n"
     ]
    }
   ],
   "source": [
    "# AUTHENTICATION CONFIGURATION\n",
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from getpass import getpass\n",
    "\n",
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(\"~/.netrc\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "    del netrcDir\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}.netrc | chmod og-rw {0}.netrc | echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n",
    "    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n",
    "    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n",
    "    del homeDir\n",
    "\n",
    "# Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n",
    "    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n",
    "    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n",
    "    del homeDir\n",
    "del urs, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stac = 'https://cmr.earthdata.nasa.gov/stac/' # CMR-STAC API Endpoint\n",
    "stac_response = r.get(stac).json()            # Call the STAC API endpoint\n",
    "stac_lp = [s for s in stac_response['links'] if 'LP' in s['title']]  \n",
    "lp_cloud = r.get([s for s in stac_lp if s['title'] == 'LPCLOUD'][0]['href']).json()\n",
    "lp_links = lp_cloud['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables\n",
    "del stac, stac_lp, stac_response\n",
    "del lp_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 items found!\n"
     ]
    }
   ],
   "source": [
    "lp_search = [l['href'] for l in lp_links if l['rel'] == 'search'][0]  # Define the search endpoint\n",
    "search_response = r.get(f\"{lp_search}\").json()                        # Send GET request to retrieve items\n",
    "lim = 100\n",
    "search_query = f\"{lp_search}?&limit={lim}\"    # Add in a limit parameter to retrieve 100 items at a time.\n",
    "search_response = r.get(search_query).json()  # send GET request to retrieve first 100 items in the STAC collection\n",
    "\n",
    "print(f\"{len(search_response['features'])} items found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field is area inside tile to grab one tile and not overlapping ones\n",
    "# fieldtile is the area of the tile that sets the extent as the full tile\n",
    "field = gp.read_file('map.geojson')\n",
    "fieldShape = field['geometry'][0] # Define the geometry as a shapely polygon\n",
    "# Bring in the farm field region of interest\n",
    "fieldtile = gp.read_file('Tile11SLU.geojson')\n",
    "tileShape = fieldtile['geometry'][0] # Define the geometry as a shapely polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 items found!\n"
     ]
    }
   ],
   "source": [
    "bbox = f'{fieldShape.bounds[0]},{fieldShape.bounds[1]},{fieldShape.bounds[2]},{fieldShape.bounds[3]}'  # Defined from ROI bounds\n",
    "search_query2 = f\"{search_query}&bbox={bbox}\"  # Add bbox to query\n",
    "search_response = r.get(search_query2).json()                                                          # Send request\n",
    "print(f\"{len(search_response['features'])} items found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 items found!\n"
     ]
    }
   ],
   "source": [
    "date_time = \"2020-01-01T00:00:00Z/2021-01-31T23:31:12Z\"  # Define start time period / end time period\n",
    "search_query3 = f\"{search_query2}&datetime={date_time}\"  # Add to query that already includes bounding_box\n",
    "search_response = r.get(search_query3).json()            \n",
    "print(f\"{len(search_response['features'])} items found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_items = search_response['features']  # Start a list of items for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the HLSS30 items of interest:\n",
    "#search_query4 = f\"{search_query3}&concept_id={s30_id}\"\n",
    "#s30_items = r.get(search_query4).json()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the HLSL30 items of interest:\n",
    "#search_query5 = f\"{search_query3}&concept_id={l30_id}\"\n",
    "#l30_items = r.get(search_query5).json()['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hls_items = s30_items + l30_items  # Combine the S30 ad L30 items:\n",
    "#hls_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del bbox, date_time, field, lim, lp_links, lp_search, search_query, search_query2, search_query3, search_response, s30_items, l30_items  # Remove search_query4, search_query5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good 11SLU at index 0 is 2% cloudy.\n",
      "good 11SLU at index 1 is 19% cloudy.\n",
      "good 11SLU at index 2 is 1% cloudy.\n",
      "good 11SLU at index 3 is 14% cloudy.\n",
      "good 11SLU at index 4 is 24% cloudy.\n",
      "good 11SLU at index 5 is 86% cloudy.\n",
      "good 11SLU at index 6 is 0% cloudy.\n",
      "good 11SLU at index 7 is 3% cloudy.\n",
      "good 11SLU at index 8 is 0% cloudy.\n",
      "good 11SLU at index 9 is 3% cloudy.\n",
      "good 11SLU at index 10 is 58% cloudy.\n",
      "good 11SLU at index 11 is 64% cloudy.\n",
      "good 11SLU at index 12 is 0% cloudy.\n",
      "good 11SLU at index 13 is 0% cloudy.\n",
      "good 11SLU at index 14 is 0% cloudy.\n",
      "good 11SLU at index 15 is 1% cloudy.\n",
      "good 11SLU at index 16 is 86% cloudy.\n",
      "good 11SLU at index 17 is 4% cloudy.\n",
      "good 11SLU at index 18 is 0% cloudy.\n",
      "good 11SLU at index 19 is 0% cloudy.\n",
      "good 11SLU at index 20 is 14% cloudy.\n",
      "good 11SLU at index 21 is 21% cloudy.\n",
      "good 11SLU at index 22 is 3% cloudy.\n",
      "good 11SLU at index 23 is 1% cloudy.\n",
      "good 11SLU at index 24 is 0% cloudy.\n",
      "good 11SLU at index 25 is 2% cloudy.\n",
      "good 11SLU at index 26 is 51% cloudy.\n",
      "good 11SLU at index 27 is 4% cloudy.\n",
      "good 11SLU at index 28 is 81% cloudy.\n",
      "good 11SLU at index 29 is 39% cloudy.\n",
      "good 11SLU at index 30 is 81% cloudy.\n",
      "good 11SLU at index 31 is 34% cloudy.\n",
      "good 11SLU at index 32 is 41% cloudy.\n",
      "good 11SLU at index 33 is 1% cloudy.\n",
      "good 11SLU at index 34 is 15% cloudy.\n",
      "good 11SLU at index 35 is 88% cloudy.\n",
      "good 11SLU at index 36 is 48% cloudy.\n",
      "good 11SLU at index 37 is 0% cloudy.\n",
      "good 11SLU at index 38 is 85% cloudy.\n",
      "good 11SLU at index 39 is 28% cloudy.\n",
      "good 11SLU at index 40 is 21% cloudy.\n",
      "good 11SLU at index 41 is 0% cloudy.\n",
      "good 11SLU at index 42 is 89% cloudy.\n",
      "good 11SLU at index 43 is 17% cloudy.\n",
      "good 11SLU at index 44 is 1% cloudy.\n",
      "good 11SLU at index 45 is 0% cloudy.\n",
      "good 11SLU at index 46 is 98% cloudy.\n",
      "good 11SLU at index 47 is 13% cloudy.\n",
      "good 11SLU at index 48 is 96% cloudy.\n",
      "good 11SLU at index 49 is 79% cloudy.\n",
      "good 11SLU at index 50 is 91% cloudy.\n"
     ]
    }
   ],
   "source": [
    "for i, h in enumerate(search_response['features']):\n",
    "    if h['assets']['browse']['href'].split('/')[5][9:14] == '11SLU':\n",
    "        print(\"good\", h['assets']['browse']['href'].split('/')[5][9:14] + f\" at index {i} is {h['properties']['eo:cloud_cover']}% cloudy.\")\n",
    "    else:\n",
    "        print(\"bad\", h['assets']['browse']['href'].split('/')[5][9:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL configurations used to successfully access LP DAAC Cloud Assets via vsicurl \n",
    "gdal.SetConfigOption(\"GDAL_HTTP_UNSAFESSL\", \"YES\")\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','YES')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hls_items[0]\n",
    "#h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLU.2020192T183921.v1.5.Fmask.tif\n",
      "https://lpdaac.earthdata.nasa.gov/lp-prod-protected/HLSS30.015/HLS.S30.T11SLU.2020192T183921.v1.5.B8A.tif\n"
     ]
    }
   ],
   "source": [
    "evi_band_links = []\n",
    "\n",
    "# Define which HLS product is being accessed\n",
    "if h['assets']['browse']['href'].split('/')[4] == 'HLSS30.015':\n",
    "    evi_bands = ['B8A', 'Fmask'] # NIR for S30\n",
    "else:\n",
    "    evi_bands = ['B05', 'Fmask'] # NIR for L30\n",
    "\n",
    "# Subset the assets in the item down to only the desired bands\n",
    "for a in h['assets']: \n",
    "    if any(b == a for b in evi_bands):\n",
    "        evi_band_links.append(h['assets'][a]['href'])\n",
    "for e in evi_band_links: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The COGs have been loaded into memory!\n"
     ]
    }
   ],
   "source": [
    "# Use vsicurl to load the data directly into memory (be patient, may take a few seconds)\n",
    "for e in evi_band_links:\n",
    "    if e.rsplit('.', 2)[-2] == evi_bands[0]: # NIR index\n",
    "        nir = rio.open(e)\n",
    "    elif e.rsplit('.', 2)[-2] == evi_bands[1]: # Fmask index\n",
    "        fmask = rio.open(e)\n",
    "    else:\n",
    "        print('none')\n",
    "print(\"The COGs have been loaded into memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_CRS = Proj('+proj=longlat +datum=WGS84 +no_defs', preserve_units=True)  # Source coordinate system of the ROI\n",
    "utm = pyproj.Proj(nir.crs)                                                  # Destination coordinate system\n",
    "project = pyproj.Transformer.from_proj(geo_CRS, utm)                        # Set up the transformation\n",
    "fsUTM = transform(project.transform, tileShape)                            # Apply reprojection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmask_array, _ = rio.mask.mask(fmask, [fsUTM], crop=False)  # Load in the Quality data\n",
    "bitword_order = (1, 1, 1, 1, 1, 1, 2)  # set the number of bits per bitword\n",
    "num_bitwords = len(bitword_order)      # Define the number of bitwords based on your input above\n",
    "total_bits = sum(bitword_order)        # Should be 8, 16, or 32 depending on datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qVals = list(np.unique(fmask_array))  # Create a list of unique values that need to be converted to binary and decoded\n",
    "all_bits = list()\n",
    "goodQuality = []\n",
    "for v in qVals:\n",
    "    all_bits = []\n",
    "    bits = total_bits\n",
    "    i = 0\n",
    "    \n",
    "    # Convert to binary based on the values and # of bits defined above:\n",
    "    bit_val = format(v, 'b').zfill(bits)\n",
    "    #print('\\n' + str(v) + ' = ' + str(bit_val))\n",
    "    all_bits.append(str(v) + ' = ' + str(bit_val))\n",
    "\n",
    "    # Go through & split out the values for each bit word based on input above:\n",
    "    for b in bitword_order:\n",
    "        prev_bit = bits\n",
    "        bits = bits - b\n",
    "        i = i + 1\n",
    "        if i == 1:\n",
    "            bitword = bit_val[bits:]\n",
    "            #print(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword)) \n",
    "        elif i == num_bitwords:\n",
    "            bitword = bit_val[:prev_bit]\n",
    "            #print(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "        else:\n",
    "            bitword = bit_val[bits:prev_bit]\n",
    "            #print(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "            \n",
    "    # 2, 4, 5, 6 are the bits used. All 4 should = 0 if no clouds, cloud shadows were present, and pixel is not snow/ice/water\n",
    "    if int(all_bits[2].split(': ')[-1]) + int(all_bits[4].split(': ')[-1]) + \\\n",
    "    int(all_bits[5].split(': ')[-1]) + int(all_bits[6].split(': ')[-1]) == 0:\n",
    "        goodQuality.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndvi(red, nir, Denom):\n",
    "      return ((nir - red) / (Denom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 21:45:19\n",
      "2021-02-12 21:45:19\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 1 of 51\n",
      "2021-02-12 21:45:38\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-21176e722086>:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return ((nir - red) / (Denom))\n",
      "<ipython-input-23-21176e722086>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return ((nir - red) / (Denom))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fmask\n",
      "Processing file 2 of 51\n",
      "2021-02-12 21:46:08\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 3 of 51\n",
      "2021-02-12 21:54:17\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 4 of 51\n",
      "2021-02-12 21:54:45\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 5 of 51\n",
      "2021-02-12 21:55:12\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 6 of 51\n",
      "2021-02-12 22:05:16\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 7 of 51\n",
      "2021-02-12 22:05:44\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 8 of 51\n",
      "2021-02-12 22:16:42\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 9 of 51\n",
      "2021-02-12 22:17:09\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 10 of 51\n",
      "2021-02-12 22:27:38\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 11 of 51\n",
      "2021-02-12 22:28:04\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 12 of 51\n",
      "2021-02-12 22:38:41\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 13 of 51\n",
      "2021-02-12 22:39:08\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 14 of 51\n",
      "2021-02-12 22:49:53\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 15 of 51\n",
      "2021-02-12 22:50:23\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 16 of 51\n",
      "2021-02-12 23:01:09\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 17 of 51\n",
      "2021-02-12 23:01:35\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 18 of 51\n",
      "2021-02-12 23:12:15\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 19 of 51\n",
      "2021-02-12 23:13:08\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 20 of 51\n",
      "2021-02-12 23:24:20\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 21 of 51\n",
      "2021-02-12 23:24:49\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 22 of 51\n",
      "2021-02-12 23:35:45\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 23 of 51\n",
      "2021-02-12 23:36:23\n",
      "band\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 24 of 51\n",
      "2021-02-12 23:46:48\n",
      "band\n",
      "cog\n",
      "null\n",
      "ndvi\n",
      "fmask\n",
      "Processing file 25 of 51\n",
      "2021-02-12 23:47:23\n",
      "band\n",
      "cog\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "t1 = datetime.datetime.now()\n",
    "print (t1.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# Now put it all together and loop through each of the files, visualize, calculate statistics on EVI, and export\n",
    "for j, h in enumerate(hls_items):\n",
    "    try:\n",
    "        timetest = datetime.datetime.now()\n",
    "        print (timetest.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        evi_band_links = []\n",
    "        if h['assets']['browse']['href'].split('/')[4] == 'HLSS30.015':\n",
    "            evi_bands = ['B8A', 'B04', 'B02', 'Fmask'] # NIR RED BLUE FMASK\n",
    "        else:\n",
    "            evi_bands = ['B05', 'B04', 'B02', 'Fmask'] # NIR RED BLUE FMASK\n",
    "        for a in h['assets']: \n",
    "            if any(b == a for b in evi_bands):\n",
    "                evi_band_links.append(h['assets'][a]['href'])\n",
    "        print('band')\n",
    "        # Use vsicurl to load the data directly into memory (be patient, may take a few seconds)\n",
    "        for e in evi_band_links:\n",
    "            if e.rsplit('.', 2)[-2] == evi_bands[0]: # NIR index\n",
    "                nir = rio.open(e)\n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[1]: # red index\n",
    "                red = rio.open(e)\n",
    "            #elif e.rsplit('.', 2)[-2] == evi_bands[2]: # blue index\n",
    "                #blue = rio.open(e)\n",
    "            elif e.rsplit('.', 2)[-2] == evi_bands[3]: # fmask index\n",
    "                fmask = rio.open(e)\n",
    "        print('cog')\n",
    "        # load data and scale\n",
    "        nir_array,nir_transform = rio.mask.mask(nir,[fsUTM], crop=False)\n",
    "        red_array, red_transform = rio.mask.mask(red,[fsUTM], crop=False)\n",
    "        #blue_array, blue_transform = rio.mask.mask(blue,[fsUTM], crop=False)\n",
    "        nir_scaled = nir_array[0] * nir.scales[0]\n",
    "        red_scaled = red_array[0] * red.scales[0]\n",
    "        #blue_scaled = blue_array[0] * blue.scales[0]\n",
    "        #nir_scaled[nir_array[0]==nir.nodata] = np.nan \n",
    "        #red_scaled[red_array[0]==red.nodata] = np.nan \n",
    "        #blue_scaled[blue_array[0]==blue.nodata] = np.nan \n",
    "        print('null')\n",
    "        # Generate EVI\n",
    "        Denom = red_scaled + nir_scaled\n",
    "        evi_scaled = ndvi(red_scaled, nir_scaled, Denom)\n",
    "        print('ndvi')\n",
    "        # Quality Filter the data\n",
    "        fmask_array, _ = rio.mask.mask(fmask,[fsUTM], crop=False)\n",
    "        print('fmask')\n",
    "        qVals = list(np.unique(fmask_array))\n",
    "        all_bits = list()\n",
    "        goodQuality = []\n",
    "        for v in qVals:\n",
    "            all_bits = []\n",
    "            bits = total_bits\n",
    "            i = 0\n",
    "            # Convert to binary based on the values and # of bits defined above:\n",
    "            bit_val = format(v, 'b').zfill(bits)\n",
    "            all_bits.append(str(v) + ' = ' + str(bit_val))\n",
    "\n",
    "            # Go through & split out the values for each bit word based on input above:\n",
    "            for b in bitword_order:\n",
    "                prev_bit = bits\n",
    "                bits = bits - b\n",
    "                i = i + 1\n",
    "                if i == 1:\n",
    "                    bitword = bit_val[bits:]\n",
    "                    all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword)) \n",
    "                elif i == num_bitwords:\n",
    "                    bitword = bit_val[:prev_bit]\n",
    "                    all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "                else:\n",
    "                    bitword = bit_val[bits:prev_bit]\n",
    "                    all_bits.append(' Bit Word ' + str(i) + ': ' + str(bitword))\n",
    "\n",
    "            # 2, 4, 5, 6 are the bits used. All should = 0 if no clouds, cloud shadows were present & pixel is not snow/ice/water\n",
    "            if int(all_bits[2].split(': ')[-1]) + int(all_bits[4].split(': ')[-1]) + \\\n",
    "            int(all_bits[5].split(': ')[-1]) + int(all_bits[6].split(': ')[-1]) == 0:\n",
    "                goodQuality.append(v)\n",
    "        evi_band = np.ma.MaskedArray(evi_scaled, np.in1d(fmask_array, goodQuality, invert=True))  # Apply QA mask to the EVI data\n",
    "        evi_band = np.ma.filled(evi_band, np.nan)\n",
    "        \n",
    "        # Remove any observations that are entirely fill value\n",
    "        if np.nansum(evi_band) == 0.0:\n",
    "            print(f\"File: {h['assets']['browse']['href'].split('/')[-1].rsplit('.', 1)[0]} ({h['id']}) was entirely fill values and will not be exported.\")\n",
    "            continue\n",
    "        outName = f\"{nir.name.rsplit('/', 1)[-1].split('.B')[0]}_EVI.tif\"\n",
    "        tempName = \"temp.tif\"\n",
    "        # Create output GeoTIFF with overviews\n",
    "        evi_tif = rio.open(tempName, 'w', driver='GTiff', height=evi_band.shape[0], width=evi_band.shape[1], count=1,\n",
    "                           dtype=str(evi_band.dtype), crs=nir.crs, transform=nir_transform)\n",
    "        evi_tif.write(evi_band, 1)\n",
    "        evi_tif.build_overviews([2, 4, 8], Resampling.average)\n",
    "        evi_tif.update_tags(ns='rio_overview', resampling='average')\n",
    "        \n",
    "        # Copy the profile, add tiling and compression\n",
    "        kwds = evi_tif.profile\n",
    "        kwds['tiled'] = True\n",
    "        kwds['compress'] = 'LZW'\n",
    "        evi_tif.close()\n",
    "        \n",
    "        # Open temp file and export as valid cog\n",
    "        with rio.open(tempName, 'r+') as src:\n",
    "            copy(src, outName, copy_src_overviews=True, **kwds)\n",
    "        src.close(), os.remove(tempName);\n",
    "\n",
    "    except:\n",
    "        print(f\"Unable to access file: {h['assets']['browse']['href'].split('/')[-1].rsplit('.', 1)[0]} ({h['id']})\")\n",
    "    print(f\"Processing file {j+1} of {len(hls_items)}\")\n",
    "    \n",
    "t2 = datetime.datetime.now()\n",
    "time = t2 -t1\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove variables that are no longer needed and close the files that were read in memory\n",
    "del all_bits, b, bit_val, bits, bitword, bitword_order, blue_array, blue_scaled, e, evi_band, evi_band_links, evi_bands\n",
    "del evi_scaled, fmask_array, goodQuality, h, hls_items, i, nir_array, nir_scaled, nir_transform, num_bitwords, outName\n",
    "del prev_bit, qVals, red_array, red_scaled, stac, total_bits, v\n",
    "nir.close(), red.close(), fmask.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
